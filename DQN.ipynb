{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b7b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813b3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=9,out_features=128)\n",
    "        self.layer2 = nn.Linear(in_featrures=128,out_features=128)\n",
    "        self.head = nn.Linear(in_featrures=128,out_features=21)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eeee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy pasted so that we remember to create same methods\n",
    "class NimDQNlearningAgent:\n",
    "    #Q learning agent for Nim with epsilon-greedy policy\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.1, player=0, buffer_size=10000, batch_size=64, update_cycle=500, target_network=None, policy_network=None, ReplayBuffer=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.player = player #  0 or 1\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.update_cycle = update_cycle\n",
    "        \n",
    "        #Initialize replay buffer\n",
    "        if ReplayBuffer is not None:\n",
    "            self.buffer = ReplayBuffer\n",
    "        else:\n",
    "            self.buffer = ReplayMemory(self.buffer_size)\n",
    "        \n",
    "        if policy_network is not None:\n",
    "            self.policy_network = policy_network\n",
    "        else:\n",
    "            self.policy_network = DQN()\n",
    "        if target_network is not None:\n",
    "            self.target_network = target_network\n",
    "        else:\n",
    "            self.target_network = self.policy_network\n",
    "        \n",
    "    \n",
    "    def copy(self):\n",
    "        return NimDQNlearningAgent(alpha=self.alpha, gamma=self.gamma, epsilon=self.epsilon, player=self.player, Q=self.Q, buffer_size=self.buffer_size, batch_size=self.batch_size, update_cycle=self.update_cycle, DQnetwork=self.DQN, ReplayBuffer=self.buffer)\n",
    "    \n",
    "    def randomMove(self, heaps):\n",
    "        # choose a random move from the available heaps\n",
    "        heaps_avail = [i for i in range(len(heaps)) if heaps[i] > 0]\n",
    "        chosen_heap = random.choice(heaps_avail)\n",
    "        n_obj = random.choice(range(1, heaps[chosen_heap] + 1))\n",
    "        move = [chosen_heap + 1, n_obj]\n",
    "        return move\n",
    "        \n",
    "    def act(self, heaps):\n",
    "        # take a state, return the action with max Q-value\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.randomMove(heaps)\n",
    "        else:\n",
    "            move = np.unravel_index(np.argmax(self.Q[heaps[0], heaps[1], heaps[2], :, :]), self.Q[heaps[0], heaps[1], heaps[2], :, :].shape)\n",
    "            return [move[0] + 1, move[1] + 1]\n",
    "        \n",
    "    def update(self, oldHeaps, move, newHeaps, reward):\n",
    "        \n",
    "    def update\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
