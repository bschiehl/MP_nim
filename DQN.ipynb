{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b7b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from nim_env import NimEnv, OptimalPlayer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813b3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "#State and next state vectors (datatype tensor) of 9 bits\n",
    "#Action index of the made action\n",
    "#Reward -1,0,1\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1dc1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=9,out_features=128)\n",
    "        self.layer2 = nn.Linear(in_features=128,out_features=128)\n",
    "        self.head = nn.Linear(in_features=128,out_features=21)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        #x size 64x9\n",
    "        x = F.relu(self.layer1(x)) #64x128\n",
    "        x = F.relu(self.layer2(x)) #64x128\n",
    "        return self.head(x) #64x21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "282fbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaps_to_bits(heaps):\n",
    "    #Return vector of length 9\n",
    "    #Heap numbers from 1\n",
    "    #If [0,0,0] -> None\n",
    "    if heaps == [0,0,0]:\n",
    "        return None\n",
    "    else:\n",
    "        return [int(bin(heaps[0])[2:].zfill(3)[-3]), int(bin(heaps[0])[2:].zfill(3)[-2]), int(bin(heaps[0])[2:].zfill(3)[-1]),\n",
    "               int(bin(heaps[1])[2:].zfill(3)[-3]), int(bin(heaps[1])[2:].zfill(3)[-2]), int(bin(heaps[1])[2:].zfill(3)[-1]),\n",
    "               int(bin(heaps[2])[2:].zfill(3)[-3]), int(bin(heaps[2])[2:].zfill(3)[-2]), int(bin(heaps[2])[2:].zfill(3)[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b509fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy pasted so that we remember to create same methods\n",
    "class NimDQNlearningAgent:\n",
    "    #Q learning agent for Nim with epsilon-greedy policy\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.1, player=0, buffer_size=10000, batch_size=64, target_network=None, policy_network=None, ReplayBuffer=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.player = player #  0 or 1\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Initialize replay buffer\n",
    "        if ReplayBuffer is not None:\n",
    "            self.buffer = ReplayBuffer\n",
    "        else:\n",
    "            self.buffer = ReplayMemory(self.buffer_size)\n",
    "        \n",
    "        if policy_network is not None:\n",
    "            self.policy_network = policy_network\n",
    "        else:\n",
    "            self.policy_network = DQN()\n",
    "        if target_network is not None:\n",
    "            self.target_network = target_network\n",
    "        else:\n",
    "            self.target_network = DQN()\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "            \n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=0.0005)\n",
    "    \n",
    "    def copy(self):\n",
    "        return NimDQNlearningAgent(alpha=self.alpha, gamma=self.gamma, epsilon=self.epsilon, player=self.player, buffer_size=self.buffer_size, batch_size=self.batch_size, update_cycle=self.update_cycle, target_network=self.target_network, policy_network=policy_network, ReplayBuffer=self.buffer)\n",
    "    \n",
    "    def randomMove(self, heaps):\n",
    "        # choose a random move from the available heaps\n",
    "        heaps_avail = [i for i in range(len(heaps)) if heaps[i] > 0]\n",
    "        chosen_heap = random.choice(heaps_avail)\n",
    "        n_obj = random.choice(range(1, heaps[chosen_heap] + 1))\n",
    "        move = [chosen_heap + 1, n_obj]\n",
    "        return move\n",
    "    \n",
    "    def act(self, heaps):\n",
    "        # take a state, return the action with max Q-value\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.randomMove(heaps)\n",
    "        else:\n",
    "            #Q values from policy network\n",
    "            s = heaps_to_bits(heaps)\n",
    "            Q = self.policy_network(torch.tensor(s).float())\n",
    "            i_max = np.argmax(Q.detach().numpy())\n",
    "            #Transform into heap number and number of sticks\n",
    "            move = [i_max//7+1, i_max%7+1]\n",
    "            return move\n",
    "        \n",
    "    def update(self, oldHeaps, move, newHeaps, reward):\n",
    "        old_state = torch.tensor(heaps_to_bits(oldHeaps)).float()\n",
    "        new_state = heaps_to_bits(newHeaps)\n",
    "        if new_state is not None:\n",
    "            new_state = torch.tensor(new_state).float()\n",
    "        index_of_move = torch.tensor([(move[0]-1)*7 + move[1]-1]).float()\n",
    "        reward = torch.tensor([reward]).float()\n",
    "        \n",
    "        #Save to replay buffer\n",
    "        self.buffer.push(old_state,index_of_move,new_state,reward)\n",
    "        \n",
    "        #Sample from replay if possible and update policy netwok\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        non_final_next_states = non_final_next_states.view(int(non_final_next_states.size()[0]/9),9)\n",
    "        state_batch = torch.cat(batch.state).view(self.batch_size,9)\n",
    "        action_batch = torch.cat(batch.action).view(self.batch_size,1)\n",
    "        reward_batch = torch.cat(batch.reward).view(self.batch_size,1)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_network(state_batch) #.gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size)\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #for param in policy_net.parameters():\n",
    "            #param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4600fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilonN(gameNumber, exploreGames, epsilonMin=0.1, epsilonMax=0.8):\n",
    "    return max(epsilonMin, epsilonMax * (1 - gameNumber/exploreGames))\n",
    "\n",
    "def playGames(env, player1, player2, decreaseExploration, exploGames=None, numGames=20000, update_cycle=500):\n",
    "    reward_sum = 0\n",
    "    games_won = 0\n",
    "    avg_rewards = []\n",
    "    M_opts = []\n",
    "    M_rands = []\n",
    "    for i in range(numGames):\n",
    "        if decreaseExploration:\n",
    "            player1.epsilon = get_epsilonN(i+1, exploGames)\n",
    "        env.reset()\n",
    "        lastActionState = None\n",
    "        lastAction = None\n",
    "        end_with_unavailable = False\n",
    "        while not env.end:\n",
    "            heaps, _, __ = env.observe()\n",
    "            if env.current_player == player2.player:\n",
    "                move = player2.act(heaps)\n",
    "                newHeaps, end, winner = env.step(move)\n",
    "            else:\n",
    "                if (lastActionState is not None and lastAction is not None):\n",
    "                    reward = env.reward(player=player1.player)\n",
    "                    # update Q-values for the last state where the Q-learning agent was playing\n",
    "                    player1.update(lastActionState, lastAction, newHeaps, reward)\n",
    "                lastActionState = heaps.copy()\n",
    "                move = player1.act(heaps)\n",
    "                lastAction = move\n",
    "                #Possibility to have unavailable action\n",
    "                #If error, give reward -1 and end the game\n",
    "                try:\n",
    "                    newHeaps, end, winner = env.step(move)\n",
    "                except AssertionError:\n",
    "                    reward = -1\n",
    "                    newHeaps = [0,0,0]\n",
    "                    end = True\n",
    "                    winner = player2.player\n",
    "                    end_with_unavailable = True\n",
    "\n",
    "            if end:\n",
    "                if not end_with_unavailable:\n",
    "                    reward = env.reward(player=player1.player)\n",
    "                player1.update(lastActionState, lastAction, newHeaps, reward)\n",
    "                reward_sum += reward\n",
    "                if reward > 0:\n",
    "                    games_won += 1\n",
    "                if ( (i+1) % 250 == 0):\n",
    "                    #compute statistics\n",
    "                    avg_rewards.append(reward_sum / 250)\n",
    "                    reward_sum = 0\n",
    "                    M_opt = calc_M(player1, 0)\n",
    "                    M_rand = calc_M(player1, 1)\n",
    "                    M_opts.append(M_opt)\n",
    "                    M_rands.append(M_rand)\n",
    "                break\n",
    "        # switch players\n",
    "        player1.player = 1 - player1.player\n",
    "        player2.player = 1 - player2.player\n",
    "        \n",
    "        #Every 500 game udate targer\n",
    "        if i+1 % update_cycle == 0:\n",
    "            player1.update_target()\n",
    "        \n",
    "    \n",
    "    print(f\"Winrate: {games_won/numGames}\")\n",
    "    return avg_rewards, M_opts, M_rands\n",
    "\n",
    "def calc_M(player, epsilon, N=500):\n",
    "    player_test = player.copy()\n",
    "    player_test.player = 0\n",
    "    player_test.epsilon = 0\n",
    "    player_opt = OptimalPlayer(epsilon=epsilon, player=1)\n",
    "    games_won = 0\n",
    "    for i in range(N):\n",
    "        env = NimEnv(seed=i)\n",
    "        while not env.end:\n",
    "            heaps, _, __ = env.observe()\n",
    "            if env.current_player == player_opt.player:\n",
    "                move = player_opt.act(heaps)\n",
    "                newHeaps, end, winner = env.step(move)\n",
    "            else:\n",
    "                move = player_test.act(heaps)\n",
    "                newHeaps, end, winner = env.step(move)\n",
    "            if end:\n",
    "                reward = env.reward(player=player_test.player)\n",
    "                if reward > 0:\n",
    "                    games_won += 1\n",
    "                break\n",
    "\n",
    "        player_opt.player = 1 - player_opt.player\n",
    "        player_test.player = 1 - player_test.player\n",
    "    \n",
    "    return (2* games_won - N) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d096fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NimEnv(seed = 10)\n",
    "random.seed(0)\n",
    "params = {'legend.fontsize': 'large',\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "mpl.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f8e133ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noora\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:921: UserWarning: Using a target size (torch.Size([64, 1, 64])) that is different to the input size (torch.Size([64, 1, 21])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (21) must match the size of tensor b (64) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9916/104878901.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplayer_Q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNimDQNlearningAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplayer_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimalPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mavg_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_opts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_rands\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayGames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer_Q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecreaseExploration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavg_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9916/2320832164.py\u001b[0m in \u001b[0;36mplayGames\u001b[1;34m(env, player1, player2, decreaseExploration, exploGames, numGames, update_cycle)\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mend_with_unavailable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mplayer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlastActionState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlastAction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewHeaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0mreward_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9916/2555715851.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, oldHeaps, move, newHeaps, reward)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# Compute Huber loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSmoothL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_action_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_state_action_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# Optimize the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[1;34m(input, target, size_average, reduce, reduction, beta)\u001b[0m\n\u001b[0;32m   3167\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3169\u001b[1;33m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3170\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (21) must match the size of tensor b (64) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Question 11\n",
    "player_Q = NimDQNlearningAgent(player=0, epsilon=0.2)\n",
    "player_opt = OptimalPlayer(epsilon=0.5, player=1)\n",
    "avg_rewards, M_opts, M_rands = playGames(env, player_Q, player_opt, decreaseExploration=False)\n",
    "plt.figure(figsize=(10, 6), dpi=80)\n",
    "ax = sns.lineplot(x=list(range(250, 20001, 250)), y=avg_rewards)\n",
    "ax.set(ylabel=\"Average reward\", xlabel=\"Number of games\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b3060c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2]],\n",
       "\n",
       "        [[3, 4]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = heaps_to_bits([3,0,0])\n",
    "t = torch.tensor([1,2,3,4]).view(2,2)\n",
    "t.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54853c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
