{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b7b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813b3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features=9,out_features=128)\n",
    "        self.layer2 = nn.Linear(in_featrures=128,out_features=128)\n",
    "        self.head = nn.Linear(in_featrures=128,out_features=21)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        #x = x.to(device)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaps_to_bits(heaps):\n",
    "    #Return vector of length 9\n",
    "    #Heap numbers from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy pasted so that we remember to create same methods\n",
    "class NimDQNlearningAgent:\n",
    "    #Q learning agent for Nim with epsilon-greedy policy\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.1, player=0, buffer_size=10000, batch_size=64, update_cycle=500, target_network=None, policy_network=None, ReplayBuffer=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.player = player #  0 or 1\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.update_cycle = update_cycle\n",
    "        \n",
    "        #Initialize replay buffer\n",
    "        if ReplayBuffer is not None:\n",
    "            self.buffer = ReplayBuffer\n",
    "        else:\n",
    "            self.buffer = ReplayMemory(self.buffer_size)\n",
    "        \n",
    "        if policy_network is not None:\n",
    "            self.policy_network = policy_network\n",
    "        else:\n",
    "            self.policy_network = DQN()\n",
    "        if target_network is not None:\n",
    "            self.target_network = target_network\n",
    "        else:\n",
    "            self.target_network = DQN()\n",
    "            target_network.load_state_dict(policy_network.state_dict())\n",
    "            \n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=0.0005)\n",
    "    \n",
    "    def copy(self):\n",
    "        return NimDQNlearningAgent(alpha=self.alpha, gamma=self.gamma, epsilon=self.epsilon, player=self.player, buffer_size=self.buffer_size, batch_size=self.batch_size, update_cycle=self.update_cycle, target_network=self.target_network, policy_network=policy_network, ReplayBuffer=self.buffer)\n",
    "    \n",
    "    def randomMove(self, heaps):\n",
    "        # choose a random move from the available heaps\n",
    "        heaps_avail = [i for i in range(len(heaps)) if heaps[i] > 0]\n",
    "        chosen_heap = random.choice(heaps_avail)\n",
    "        n_obj = random.choice(range(1, heaps[chosen_heap] + 1))\n",
    "        move = [chosen_heap + 1, n_obj]\n",
    "        return move\n",
    "    \n",
    "    def act(self, heaps):\n",
    "        # take a state, return the action with max Q-value\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.randomMove(heaps)\n",
    "        else:\n",
    "            #Q values from policy network\n",
    "            s = heaps_to_bits(heaps)\n",
    "            Q = self.policy_network(s)\n",
    "            i_max = np.argmax(Q)\n",
    "            #Transform into heap number and number of sticks\n",
    "            move = [i_max//7+1, i_max%7+1]\n",
    "            return move\n",
    "        \n",
    "    def update(self, oldHeaps, move, newHeaps, reward):\n",
    "        old_state = heaps_to_bits(oldHeap)\n",
    "        new_state = heaps_to_bits(newHeaps)\n",
    "        index_of_move = move[0]\n",
    "        \n",
    "        #Save to replay buffer\n",
    "        self.buffer.push([old_state,index_of_move,new_state,reward])\n",
    "        \n",
    "        #Sample from replay if possible and update policy netwok\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size)\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #for param in policy_net.parameters():\n",
    "            #param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
